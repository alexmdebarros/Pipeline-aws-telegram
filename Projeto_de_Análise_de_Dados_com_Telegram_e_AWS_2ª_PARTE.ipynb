{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## A Infraestrutura na Nuvem: Ingestão de Dados\n",
        "\n",
        "A próxima etapa foi configurar a infraestrutura na AWS para receber e armazenar os dados do Telegram. Criei um bucket no S3 chamado `ebac-42-exercicio-datalake-raw`, onde os dados brutos seriam armazenados em formato JSON.\n",
        "\n",
        "Em seguida, criei uma função no Lambda em Python para receber as mensagens do Telegram e salvá-las no bucket S3. O código da função era o seguinte:\n",
        "\n",
        "``` python\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "import boto3\n",
        "\n",
        "\n",
        "def lambda_handler(event: dict, context: dict) -> dict:\n",
        "\n",
        "  '''\n",
        "  Recebe uma mensagens do Telegram via AWS API Gateway, verifica no\n",
        "  seu conteúdo se foi produzida em um determinado grupo e a escreve,\n",
        "  em seu formato original JSON, em um bucket do AWS S3.\n",
        "  '''\n",
        "\n",
        "  # vars de ambiente criada nas configurações do lambda\n",
        "\n",
        "  BUCKET = os.environ['AWS_S3_BUCKET']\n",
        "  TELEGRAM_CHAT_ID = int(os.environ['TELEGRAM_CHAT_ID'])\n",
        "\n",
        "  # vars lógicas\n",
        "\n",
        "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
        "  date = datetime.now(tzinfo).strftime('%Y-%m-%d')\n",
        "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
        "\n",
        "  filename = f'{timestamp}.json'\n",
        "\n",
        "  # código principal\n",
        "\n",
        "  client = boto3.client('s3')\n",
        "  \n",
        "  try:\n",
        "\n",
        "    message = json.loads(event[\"body\"])\n",
        "    #message = event\n",
        "    chat_id = message[\"message\"][\"chat\"][\"id\"]\n",
        "\n",
        "    if chat_id == TELEGRAM_CHAT_ID:\n",
        "\n",
        "      with open(f\"/tmp/{filename}\", mode='w', encoding='utf8') as fp:\n",
        "        json.dump(message, fp)\n",
        "\n",
        "      client.upload_file(f'/tmp/{filename}', BUCKET, f'telegram/context_date={date}/{filename}')\n",
        "\n",
        "  except Exception as exc:\n",
        "      logging.error(msg=exc)\n",
        "      return dict(statusCode=\"500\")\n",
        "\n",
        "  else:\n",
        "      return dict(statusCode=\"200\")\n",
        "```\n",
        "\n",
        "Configurei as permissões do IAM para que a função Lambda pudesse acessar o bucket S3.\n",
        "\n",
        "Para conectar o Telegram à função Lambda, criei uma API no API Gateway. Configurei o webhook do bot para enviar as mensagens para a API, utilizando o método POST.\n",
        "\n",
        "<img width='800px' src='https://github.com/alexmdebarros/Pipeline-aws-telegram/blob/main/api-gateway.png?raw=true'>\n",
        "\n",
        "# A Transformação dos Dados: ETL\n",
        "\n",
        "Com os dados brutos (JSON) armazenados no S3, era hora de transformá-los em um formato mais adequado para análise. Criei outro bucket no S3 chamado `ebac-42-exercicio-datalake-enriched`, onde os dados enriquecidos seriam armazenados em formato `PARQUET`.\n",
        "\n",
        "Criei outra função no Lambda em Python para processar os dados brutos e transformá-los em formato `PARQUET`. O código da função era o seguinte:\n",
        "\n",
        "```PYTHON\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import boto3\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "def lambda_handler(event: dict, context: dict) -> bool:\n",
        "    RAW_BUCKET = os.environ['AWS_S3_BUCKET']\n",
        "    ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n",
        "\n",
        "    tzinfo = timezone(offset=timedelta(hours=-3))\n",
        "    date = (datetime.now(tzinfo) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "    timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
        "\n",
        "    table = None\n",
        "    client = boto3.client('s3')\n",
        "\n",
        "    try:\n",
        "        prefix = f'telegram/context_date={date}'\n",
        "        logger.info(f\"Listando objetos em {RAW_BUCKET}/{prefix}\")\n",
        "        response = client.list_objects_v2(Bucket=RAW_BUCKET, Prefix=prefix)\n",
        "\n",
        "        if 'Contents' not in response:\n",
        "            logger.info(f\"Nenhum arquivo encontrado para {prefix}\")\n",
        "            return True\n",
        "\n",
        "        for content in response['Contents']:\n",
        "            key = content['Key']\n",
        "            local_file = f\"/tmp/{key.split('/')[-1]}\"\n",
        "            logger.info(f\"Baixando {key} para {local_file}\")\n",
        "            client.download_file(RAW_BUCKET, key, local_file)\n",
        "\n",
        "            try:\n",
        "                with open(local_file, mode='r', encoding='utf8') as fp:\n",
        "                    data = json.load(fp)\n",
        "                    if \"message\" in data:\n",
        "                        parsed_data = parse_data(data=data[\"message\"])\n",
        "                        iter_table = pa.Table.from_pydict(mapping=parsed_data)\n",
        "\n",
        "                        if table:\n",
        "                            table = pa.concat_tables([table, iter_table])\n",
        "                        else:\n",
        "                            table = iter_table\n",
        "\n",
        "                    else:\n",
        "                        logger.warning(f\"Arquivo {key} não possui a chave 'message'.\")\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.error(f\"Erro ao decodificar JSON de {key}: {e}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erro ao processar arquivo {key}: {e}\")\n",
        "\n",
        "        if table:\n",
        "            parquet_file = f'/tmp/{timestamp}.parquet'\n",
        "            pq.write_table(table=table, where=parquet_file)\n",
        "            s3_key = f\"telegram/context_date={date}/{timestamp}.parquet\"\n",
        "            logger.info(f\"Enviando {parquet_file} para {ENRICHED_BUCKET}/{s3_key}\")\n",
        "            client.upload_file(parquet_file, ENRICHED_BUCKET, s3_key)\n",
        "            return True\n",
        "        else:\n",
        "            logger.info(\"Nenhum dado processado.\")\n",
        "            return True\n",
        "\n",
        "    except Exception as exc:\n",
        "        logger.error(f\"Erro geral: {exc}\")\n",
        "        return False\n",
        "\n",
        "def parse_data(data: dict) -> dict:\n",
        "    parsed_data = {}\n",
        "    for key, value in data.items():\n",
        "        if key == 'from':\n",
        "            for k, v in data[key].items():\n",
        "                if k in ['id', 'is_bot', 'first_name']:\n",
        "                    parsed_data[f\"user_{k}\"] = [v]\n",
        "        elif key == 'chat':\n",
        "            for k, v in data[key].items():\n",
        "                if k in ['id', 'type']:\n",
        "                    parsed_data[f\"chat_{k}\"] = [v]\n",
        "        elif key in ['message_id', 'date', 'text']:\n",
        "            parsed_data[key] = [value]\n",
        "\n",
        "    # Garante que a coluna 'text' sempre exista e seja uma lista de strings.\n",
        "    if 'text' not in parsed_data:\n",
        "        parsed_data['text'] = [None]\n",
        "    else:\n",
        "        if parsed_data['text'][0] is None:\n",
        "            parsed_data['text'] = [None]\n",
        "        else:\n",
        "            if not isinstance(parsed_data['text'][0], str):\n",
        "                parsed_data['text'] = [str(parsed_data['text'][0])]\n",
        "\n",
        "    return parsed_data\n",
        "\n",
        "```\n",
        "\n",
        "Configurei as permissões do IAM para que a função Lambda pudesse acessar os buckets S3. Também configurei o timeout da função para 5 minutos e adicionei a layer com o pacote `PyArrow`.\n",
        "\n",
        "Para automatizar o processo, criei uma regra no EventBridge, agendando a execução da função Lambda diariamente, à meia-noite.\n",
        "\n",
        "<img width='800px' src='https://github.com/alexmdebarros/Pipeline-aws-telegram/blob/main/eventbridge.png?raw=true'>"
      ],
      "metadata": {
        "id": "dU6J5UR3SDEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Descoberta dos Insights: Análise e Visualização\n",
        "\n",
        "Com os dados enriquecidos e armazenados no S3, era hora de explorar os insights. Criei uma tabela no Athena que apontava para os dados no bucket `ebac-42-exercicio-datalake-enriched`.\n",
        "\n",
        "Executei o comando `MSCK REPAIR TABLE telegram` para carregar as partições da tabela.\n",
        "\n",
        "Utilizei as seguintes queries SQL para explorar os dados:\n",
        "\n",
        "**Contagem de mensagens por dia:**\n",
        "\n",
        "`SELECT context_date, count(*) FROM telegram\n",
        "GROUP BY context_date\n",
        "ORDER BY context_date;`\n",
        "\n",
        "Para entender a atividade do grupo ao longo do tempo, analisei a quantidade de mensagens enviadas por dia. A query agrupa os dados pela data de contexto (context_date), contando o total de mensagens por dia e ordenando os resultados em ordem crescente. A tabela gerada mostra a variação no volume de mensagens, permitindo identificar picos de interação e dias com menor atividade.\n",
        "\n",
        "<img width='800px' src='https://github.com/alexmdebarros/Pipeline-aws-telegram/blob/main/total-mens-dia.png?raw=true'>\n",
        "\n",
        "**Frequência de palavras:**\n",
        "\n",
        "`SELECT word, count(*) FROM telegram, UNNEST(SPLIT(text, ' ')) AS t(word) WHERE text NOT IN ('', '') GROUP BY word ORDER BY count(*) DESC LIMIT 10;`\n",
        "\n",
        "Uma análise interessante foi a contagem das palavras mais utilizadas nas mensagens. Para isso, utilizei a função `UNNEST` para dividir o texto das mensagens em palavras individuais e contar a frequência de cada uma. O resultado revela os termos mais recorrentes nas conversas do grupo, o que pode ser útil para entender padrões de comunicação e temas mais discutidos.\n",
        "\n",
        "<img width='800px' src='https://github.com/alexmdebarros/Pipeline-aws-telegram/blob/main/frequencia-palavras-grafico.png?raw=true'>\n",
        "\n",
        "**Horário com mais mensagens enviadas:**\n",
        "\n",
        "`SELECT HOUR(FROM_UNIXTIME(date)) AS hora, COUNT(*) AS total_mensagens\n",
        "FROM telegram\n",
        "GROUP BY HOUR(FROM_UNIXTIME(date))\n",
        "ORDER BY total_mensagens DESC;`\n",
        "\n",
        "Para identificar os horários de maior atividade no grupo, extraí a hora das mensagens a partir da coluna date, convertendo o timestamp para um formato legível. Em seguida, agrupei os dados por hora e contei a quantidade de mensagens enviadas em cada período. A tabela mostra claramente os horários de pico, ajudando a entender quando há maior engajamento no grupo.\n",
        "\n",
        "<img width='800px' src='https://github.com/alexmdebarros/Pipeline-aws-telegram/blob/main/qtd-mensagens-hora.png?raw=true'>"
      ],
      "metadata": {
        "id": "Y5XMtu_UXq4M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iq_MamflYqV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}